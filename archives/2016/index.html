<!DOCTYPE HTML>

    <html lang="zh-Hans">
  
<head>
  <meta charset="utf-8">
  
  <title>归档：2016 | CastellanZhang&#39;s blog</title>
  <meta name="author" content="CastellanZhang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="CastellanZhang&#39;s blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <meta http-equiv="Content-Language" content="zh-Hans"/>
  

  <link href="/img/favicon.png" rel="icon">
  
    <link rel="apple-touch-icon" href="/img/apple-icon.png">
    <link rel="apple-touch-icon-precomposed" href="/img/apple-icon.png">
    

  <link rel="alternate" href="/atom.xml" title="CastellanZhang&#39;s blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  
  <style type="text/css">
  /* Tim Pietrusky advanced checkbox hack (Android <= 4.1.2) */
body{ -webkit-animation: bugfix infinite 1s; }
@-webkit-keyframes bugfix { from {padding:0;} to {padding:0;} }

  
  <!-- Chinese readability improvements -->
    article {font-weight: 400;letter-spacing: .01rem;}
    article .entry{line-height:2;}
  

  
    article .post-content-index .entry{max-height: 550px; overflow:hidden;}
  
</style>

  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'null', 'auto');
  ga('send', 'pageview');
 
</script>




  
    <!-- 360 Font and Baidu CDN in China -->
    
      <link href='http://fonts.useso.com/css?family=Open+Sans:300,400|Playball' rel='stylesheet' type='text/css'>
    
  <link href='http://apps.bdimg.com/libs/fontawesome/4.1.0/css/font-awesome.css' rel='stylesheet' type='text/css'>
  <script src="http://libs.baidu.com/jquery/1.11.1/jquery.min.js"></script>
  



</head>


<body>
  <header id="header" class="inner"><div class="padding">
	<div class="alignleft logo">
	  <h1><a href="/">CastellanZhang&#39;s blog</a></h1>
	</div>
	<nav id="main-nav" class="alignright">
		<input type="checkbox" id="toggle" />
		<label for="toggle" class="toggle" data-open="Main Menu" data-close="Close Menu" onclick><i class="fa fa-bars"></i></label>
	  <ul class="menu">
	    
	      <li><a href="/">Home</a></li>
	    
	      <li><a href="/archives">Archives</a></li>
	    
	    
	  </ul>
	</nav>
	<div class="clearfix"></div>
</div>
</header>
  <div id="page-heading-wrap">
  	<div class="inner">
      <div class="padding">
    		
          <h2></h2>
        
      </div>
  	</div>
  </div>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper" class="padding">
<h2 class="archive-title">2016</h2>


  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/11/22/ensembling_lagrange/">Ensembling, Lagrange</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-11-22T07:31:26.000Z">Nov 22 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="Ensembling_2C_Lagrange"><a href="#Ensembling_2C_Lagrange" class="headerlink" title="Ensembling, Lagrange"></a>Ensembling, Lagrange</h1><p>　　据我观察，名字带“拉”的人一般都很厉害，比如马拉多纳、希拉里、狄波拉、张娜拉、杜拉拉、陈法拉、尼古拉斯·赵四、地铁站的罗拉……</p>
<p>　　但跟我们今天的大神拉格朗日（Lagrange）一比就都被秒成了渣渣，因为每一个上过高数和物理的我们都被拉大神无情碾压摩擦过啊~</p>
<p>　　童鞋，请站起来回答拉格朗日中值定律是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日乘子法是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日内插公式是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日点是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日函数是啥？</p>
<p>　　童鞋，请站起来回答拉格朗日方程是啥？</p>
<p>　　……</p>
<p>　　童鞋，你肿么站不起来了童鞋？<br><br><br>　　请原谅我召唤出封印在你们心底多年的梦魇，快跟我一起踏碎时空大逃亡，继续闪回到小米数据挖掘大赛那几天。</p>
<p>　　<a href="http://castellanzhang.github.io/2016/10/16/fm_ftrl_softmax/" target="_blank" rel="external">上文</a>中已经提到我们最终的模型不止一个，而是多个DNN和FM共七八个模型的融合。</p>
<p>　　所谓模型融合，洋气一点叫Ensembling，在 <a href="http://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="external">http://mlwave.com/kaggle-ensembling-guide/</a> 一文中有详细介绍。简单来说就是综合多个单模型的输出来给出最终的结果，一般会比每个单模型的效果都好，现在已经是各大比赛的常规武器。但对于初涉江湖的小伙伴们还没有太多经验，一开始只是简单的平均，后来尝试了各个模型的输出做为LR的特征，发现没啥卵用，比赛已近尾声就没再折腾更复杂的方法，最终使用线性加权平均，即：<br>$$<br>\hat y=w_1\hat y_1+w_2\hat y_2+…+w_M\hat y_M<br>$$<br>　　其中，$w_1+w_2+…+w_M=1$</p>
<p>　　这里有个问题就是权重系数怎么定？</p>
<p>　　一开始就是拍脑袋定，单模型效果好的权重就大一点，效果差的就小一点。后来小伙伴使用暴力网格搜索的方法，融合两三个模型还行，再多就已经慢到不可接受。</p>
<p>　　我看在眼里急在心头，我们是模武双修的种族啊，怎么能只用暴力解决呢？这种目标如此鲜明灿若煌煌皓月的好问题，当然要祭出流光华丽的大模型才相得益彰呢！</p>
<h2 id="u7B2C_u4E00_u5F0F_uFF1A_u6DF7_u6C8C_u521D_u5206_u6A21_u578B_u73B0_uFF01"><a href="#u7B2C_u4E00_u5F0F_uFF1A_u6DF7_u6C8C_u521D_u5206_u6A21_u578B_u73B0_uFF01" class="headerlink" title="第一式：混沌初分模型现！"></a>第一式：混沌初分模型现！</h2><p>　　比如我们有M个单模型分类器，解决K分类问题，测试集包含N条样本，$\hat{y}_{nmk}$ 表示第m个单模型对第n条样本属于第k类的预测概率。</p>
<p>　　我们采用线性加权平均的方法，融合M个模型得到的预测概率<br>$$<br>\hat{y}_{nk}=\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk}<br>$$<br>　　比赛评价标准为logloss，我们希望融合后的模型在测试集上的logloss尽量小，所以优化目标如下：<br>$$<br>\min_{w}f(w)=\min_{w}\{-\frac{1}{N}\sum_{n=1}^{N}\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})\}<br>$$<br>　　其中$w=[w_1,..,w_{M-1}]^T$是需要求解的权重参数，我们知道权重之和要归一，所以不需要$w_M$这一变量，用$1-\sum_{m=1}^{M-1}w_m$代替即可。</p>
<p>　　问题已明确，下面来求解。</p>
<h2 id="u7B2C_u4E8C_u5F0F_uFF1A_u68AF_u5EA6_u6740_uFF01_uFF01"><a href="#u7B2C_u4E8C_u5F0F_uFF1A_u68AF_u5EA6_u6740_uFF01_uFF01" class="headerlink" title="第二式：梯度杀！！"></a>第二式：梯度杀！！</h2><p>　　这一招早已驾轻就熟，直接求梯度，然后上sgd或adagrad。单条样本的损失函数为<br>$$<br>l_n(w)=-\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})<br>$$<br>　　梯度为<br>$$<br>\frac{\partial l_n}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}-\hat{y}_{nMk}}{\hat{y}_{nk}},\quad m=1,…,M-1<br>$$<br>　　adagrad版本的代码如下，注意其中变量名和上面的公式并不完全一致，类别编号是从0到K-1（我就是这么洒脱随性~）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">n = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    g = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum-<span class="number">1</span>):</span><br><span class="line">        g[m] = (x[modelNum-<span class="number">1</span>][label]-x[m][label])/pre[label]</span><br><span class="line">        n[m] += g[m] * g[m]</span><br><span class="line">        lr = alfa/(beta+math.sqrt(n[m]))</span><br><span class="line">        w[m] -= lr * g[m]</span><br><span class="line">        wsum += w[m]</span><br><span class="line">    w[modelNum-<span class="number">1</span>] = <span class="number">1</span>-wsum</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure></p>
<p>　　输入数据的格式如下：</p>
<p><code>label score11,score12,...,score1K score21,score22,...,score2K ... scoreM1,scoreM2,...,scoreMK</code></p>
<p>　　举个例子，比如二分类4个模型的数据片段如下：</p>
<pre><code>0 0.912427,0.0875725 0.905673,0.0943273 0.911398,0.088602 0.933166,0.066834
0 0.86061,0.139389 0.865925,0.134075 0.881552,0.118448 0.872196,0.127804
1 0.364299,0.635701 0.32974,0.67026 0.323839,0.676161 0.341047,0.658953
0 0.715563,0.284437 0.713995,0.286005 0.713599,0.286401 0.713383,0.286617
0 0.948186,0.0518137 0.925587,0.0744127 0.929451,0.0705489 0.939952,0.0600485
0 0.58531,0.41469 0.588321,0.411679 0.520456,0.479544 0.671846,0.328154
1 0.0154455,0.984554 0.0150741,0.984926 0.0118866,0.988113 0.0109413,0.989059
1 0.0472074,0.952793 0.0612501,0.93875 0.065446,0.934554 0.061947,0.938053
1 0.430228,0.569772 0.366636,0.633364 0.337206,0.662794 0.5487,0.4513
0 0.97958,0.0204195 0.980348,0.0196517 0.979461,0.0205394 0.985608,0.0143915
</code></pre><p>　　我们的测试数据共200多万条，时间复杂度跟单模型数M呈线性关系，跑一次也就分分钟的事。为了验证算法，做了对比，对于3模型融合跑出的结果和暴力求解的结果一致。</p>
<p>　　问题似乎就这么完美解决了，但人生就像直流充电桩，上一秒还是70A的电流，下一秒就可能跳电。当我们融合更多模型时问题出现了，有的权重算出了负值！就像下面这样：</p>
<pre><code>0.120181463777    gender_cut_ftrl8000_val
0.0929962960391   gender_cut_ftrl_val
0.0245135332374   gender_nn_1005_val
0.269705618425    gender_cut_ftrl2000_val
0.422565675064    gender_cut_ftrl1000_val
0.233316720486    gender_newFM_val
0.0228310140994   gender_xxFM_val
-0.186110321128   gender_result_fm
</code></pre><p>导致最终模型的预测概率有可能大于1或小于0，这如何能忍，必须再创新招！</p>
<h2 id="u7B2C_u4E09_u5F0F_uFF1A_u751F_u6B7B_u7B26_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E09_u5F0F_uFF1A_u751F_u6B7B_u7B26_uFF01_uFF01_uFF01" class="headerlink" title="第三式：生死符！！！"></a>第三式：生死符！！！</h2><p>　　“这生死符一发作，一日厉害一日，奇痒剧痛递加九九八十一日，然后逐步减退，八十一日之后，又再递增，如此周而复始，永无休止。每年我派人巡行各洞各岛，赐以镇痛止痒之药，这生死符一年之内便可不发。”</p>
<p>　　好啦，不吓你啦，这是金庸老先生笔下天山童姥的生死符。我这里的生死符简单得很，顾名思义，由权重符号决定单模型生死，正的留，负的弃，留下的单模型重新计算权重，有可能又有新的负值出现，再次使用生死符，直到剩下的单模型权重全部大于等于0。</p>
<p>　　还以上面那次融合为例，最终把gender_xxFM_val和gender_result_fm都干掉了，剩下的权重为：</p>
<pre><code>0.122788741689    gender_cut_ftrl8000_val
0.0782434412719   gender_cut_ftrl_val
0.0331138038226   gender_nn_1005_val
0.24358375583     gender_cut_ftrl2000_val
0.446465338463    gender_cut_ftrl1000_val
0.0758049189234   gender_newFM_val
</code></pre><p>　　在测试集上的logloss = 0.435116，而其中单模型最好的logloss是0.436593。</p>
<p>　　相应地，这批融合在7分类问题age上效果更加明显，从最好的单模型1.35416降到1.35061。<br><br><br>　　以上便是比赛期间我们使用的全部招式。</p>
<p><br><br><br><br><br><br><br>　　憋走，故事还没结束，忘了塞纳河畔的拉格朗日了吗？</p>
<p>　　像我这等追求卓越的好青年岂能够留下不完美的话柄，虽然比赛结束了，我还是要把它彻底解决掉——快使出</p>
<h2 id="u7B2C_u56DB_u5F0F_uFF1A_u94C1_u9501_u6A2A_u6C5F_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u56DB_u5F0F_uFF1A_u94C1_u9501_u6A2A_u6C5F_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第四式：铁锁横江！！！！"></a>第四式：铁锁横江！！！！</h2><p>　　分析上面的模型，出现负值，是因为少了对w的非负约束，那就加上：<br>$$<br>\min_{w}f(w)=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})\}<br>$$<br>$$<br>s.t.\quad w_m\ge 0,\quad m=1,2,…,M-1\\<br>1-\sum_{m=1}^{M-1}w_m\ge 0\qquad\qquad<br>$$<br>　　这就成了带约束的优化问题，该怎么解呢？是时候请出拉格朗日大神了！大神蜜汁微笑，抛出一套对偶宝典，将带约束的极小化问题改造成极大极小问题，正是</p>
<h2 id="u7B2C_u4E94_u5F0F_uFF1A_u6CA7_u6D77_u4E00_u7C9F_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E94_u5F0F_uFF1A_u6CA7_u6D77_u4E00_u7C9F_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第五式：沧海一粟！！！！！"></a>第五式：沧海一粟！！！！！</h2><p>　　我们先把问题标准化，假定$f(x)$，$c_i(x)$，$h_j(x)$ 是定义在$R^n$上的连续可微函数，将下面约束最优化问题<br>$$<br>\min_{x}f(x)\\<br>s.t.\quad c_i(x)\le 0,\quad i=1,2,…,k\\<br>\quad \quad h_j(x)=0,\quad j=1,2,…,l<br>$$<br>称为原始问题。</p>
<p>　　然后引入广义拉格朗日函数<br>$$<br>L(x,\alpha,\beta)=f(x)+\sum_{i=1}^{k}\alpha_{i}c_i(x)+\sum_{j=1}^{l}\beta_{j}h_j(x)<br>$$<br>　　其中 $\alpha_i$ 和 $\beta_j$ 是拉格朗日乘子，$\alpha_i\ge 0$。然后经过一番推来导去眉来眼去，可以证明<br>$$<br>\min_x\max_{\alpha,\beta:\alpha_i\ge 0}L(x,\alpha,\beta)<br>$$<br>与原始问题具有相同的x最优解，称作广义拉格朗日函数的极小极大问题。</p>
<p>　　又有，当$f(x)$，$c_i(x)$，$h_j(x)$ 等满足一定条件时<br>$$<br>\max_{\alpha,\beta:\alpha_i\ge 0}\min_xL(x,\alpha,\beta)<br>$$<br>也与原始问题具有相同的x最优解，称作广义拉格朗日函数的极大极小问题，可以将此问题也改写成约束的形式：<br>$$<br>\max_{\alpha,\beta}\theta_D(\alpha,\beta)=\max_{\alpha,\beta}\min_xL(x,\alpha,\beta)\\<br>s.t.\qquad \alpha_i\ge 0,\quad i=1,2,…,k<br>$$<br>称作原始问题的对偶问题。</p>
<p>　　这一部分的具体细节懒得写了，估计你们也看不下去，真要有兴趣可以去看书，比如《凸优化》或者李航老师的《统计学习方法》，我上面的公式基本就是抄他的，但请注意书上附录C的KKT条件有误，(C.22)式和(C.23)式不应该包含在里面。</p>
<p>　　好啦，照此框架，来解决我们的问题：<br>$$<br>\max_{\alpha,\alpha_m\ge 0}\min_wL(w,\alpha)\\<br>=\max_{\alpha,\alpha_m\ge 0}\min_w\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M-1}w_m\hat{y}_{nmk}+(1-\sum_{m=1}^{M-1}w_m)\hat{y}_{nMk})-\sum_{m=1}^{M-1}\alpha_mw_m-\alpha_M(1-\sum_{m=1}^{M-1}w_m)\}<br>$$<br>　　极大极小公式列出来很容易，如何求解才头疼。</p>
<p>　　最早接触拉格朗日对偶是看SVM的推导，最近研究在线分配的shale算法又遇到它，发现基本都是依靠KKT条件推导，然而后面各有各的玩法，没有什么普适的方案。我也试着从KKT出发，但始终没找到什么高效的方法，一赌气，干脆舍弃KKT直接求解极大极小问题。如果哪位高人有更高明的招式，请一定传授小弟。</p>
<p>　　我自己想的招式便是以不变应万变，依然按sgd的思路，每来一条样本，先固定 $\alpha$ 不动，更新w，这是关于w的极小化问题，使用梯度下降（依然梯度杀）；然后固定w，更新 $\alpha$，这是关于 $\alpha$ 的极大化问题，使用梯度上升（可称梯云纵）。对 $\alpha$ 的非负约束也很简单，更新后如果小于0，则置为0。</p>
<p>　　如此交错曲折，在解空间的山坡上忽上忽下苦苦寻觅，“路漫漫其修远兮”，正是</p>
<h2 id="u7B2C_u516D_u5F0F_uFF1A_u4E0A_u4E0B_u6C42_u7D22_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u516D_u5F0F_uFF1A_u4E0A_u4E0B_u6C42_u7D22_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第六式：上下求索！！！！！！"></a>第六式：上下求索！！！！！！</h2><p>　　具体的梯度计算如下：<br>$$<br>\frac{\partial l_n(w|\alpha)}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}-\hat{y}_{nMk}}{\hat{y}_{nk}}-\alpha_m+\alpha_M,\quad m=1,…,M-1\\<br>\frac{\partial l_n(\alpha|w)}{\partial \alpha_m}=-w_m,\quad m=1,…,M-1\qquad\qquad\qquad\qquad\qquad\qquad\\<br>\frac{\partial l_n(\alpha|w)}{\partial \alpha_M}=-(1-\sum_{m=1}^{M-1}w_m)\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad<br>$$<br>　　adagrad代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">n = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    g = [<span class="number">0.0</span>] * (modelNum-<span class="number">1</span>)</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum-<span class="number">1</span>):</span><br><span class="line">        g[m] = (x[modelNum-<span class="number">1</span>][label]-x[m][label])/pre[label] - A[m] + A[modelNum-<span class="number">1</span>]</span><br><span class="line">        n[m] += g[m] * g[m]</span><br><span class="line">        lr = alfa/(beta+math.sqrt(n[m]))</span><br><span class="line">        w[m] -= lr * g[m]</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    w[modelNum-<span class="number">1</span>] = <span class="number">1</span>-wsum</span><br><span class="line">    gA = -w[modelNum-<span class="number">1</span>]</span><br><span class="line">    nA[modelNum-<span class="number">1</span>] += gA * gA</span><br><span class="line">    lrA = alfa/(beta+math.sqrt(nA[modelNum-<span class="number">1</span>]))</span><br><span class="line">    A[modelNum-<span class="number">1</span>] += lrA * gA</span><br><span class="line">    <span class="keyword">if</span> A[modelNum-<span class="number">1</span>] &lt; <span class="number">0</span>:</span><br><span class="line">        A[modelNum-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　看一下效果，还是以上面的gender二分类为例，结果如下：</p>
<pre><code>0.123262252313    gender_cut_ftrl8000_val
0.0782768467103   gender_cut_ftrl_val
0.0340907260294   gender_nn_1005_val
0.243746308063    gender_cut_ftrl2000_val
0.447138729994    gender_cut_ftrl1000_val
0.0627800144873   gender_newFM_val
0.00261889445352  gender_xxFM_val
0.00808622794875  gender_result_fm
</code></pre><p>　　可以看到gender_xxFM_val和gender_result_fm的权重终于不再是负数，而且很小，接近于0，跟期望的一样，最后的logloss也基本一致，等于0.435125。</p>
<p>　　说明我们的方法很给力啊！不能骄傲，继续前行。上面的方法对最后一项权重$w_M$ 做了特殊处理，不够优雅，如果我们不专门处理它，而是把权重之和归一放在约束里呢？</p>
<h2 id="u7B2C_u4E03_u5F0F_uFF1A_u4E07_u6CD5_u5F52_u4E00_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u4E03_u5F0F_uFF1A_u4E07_u6CD5_u5F52_u4E00_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第七式：万法归一！！！！！！！"></a>第七式：万法归一！！！！！！！</h2><p>　　重新设计模型<br>$$<br>\hat{y}_{nk}=\sum_{m=1}^{M}w_m\hat{y}_{nmk}<br>$$<br>$$<br>\min_{w}f(w)=<br>\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln\hat{y}_{nk}\}\\<br>=\min_{w}\{-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M}w_m\hat{y}_{nmk})\}\\<br>s.t.\qquad w_m\ge 0,\quad m=1,2,…,M\\<br>\sum_{m=1}^{M}w_m=1\qquad<br>$$<br>$$<br>L(w,\alpha,\beta)=-\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^{K}1\{y_n=k\}\ln(\sum_{m=1}^{M}w_m\hat{y}_{nmk})-\sum_{m=1}^{M}\alpha_mw_m+\beta(\sum_{m=1}^{M}w_m-1)<br>$$<br>　　梯度计算<br>$$<br>\frac{\partial l_n(w|\alpha,\beta)}{\partial w_m}=-\sum_{k=1}^{K}1\{y_n=k\}\frac{\hat{y}_{nmk}}{\hat{y}_{nk}}-\alpha_m+\beta,\quad m=1,…,M\\<br>\frac{\partial l_n(\alpha|w,\beta)}{\partial \alpha_m}=-w_m,\quad m=1,…,M\qquad\qquad\qquad\qquad\qquad\\<br>\frac{\partial l_n(\beta|w,\alpha)}{\partial \beta}=\sum_{m=1}^{M}w_m-1\qquad\qquad\qquad\qquad\qquad\qquad\qquad<br>$$<br>　　代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">nw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">B = <span class="number">0.0</span></span><br><span class="line">nB = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        gw = -x[m][label]/pre[label] - A[m] + B</span><br><span class="line">        nw[m] += gw * gw</span><br><span class="line">        lrw = alfa/(beta+math.sqrt(nw[m]))</span><br><span class="line">        w[m] -= lrw * gw</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    gB = wsum - <span class="number">1</span></span><br><span class="line">    nB += gB * gB</span><br><span class="line">    lrB = alfa/(beta+math.sqrt(nB))</span><br><span class="line">    B += lrB * gB</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　效果</p>
<pre><code>0.117368795386    gender_cut_ftrl8000_val
0.0736857356383   gender_cut_ftrl_val
0.0387213072901   gender_nn_1005_val
0.236047170656    gender_cut_ftrl2000_val
0.463029142921    gender_cut_ftrl1000_val
0.069749655191    gender_newFM_val
0.00104464602712  gender_xxFM_val
0.00131312041348  gender_result_fm
</code></pre><p>　　注意，这里权重之和并不是严格的1，而是1.000959573523，毕竟是个迭代算法，没法保证严格的约束啊……所以计算logloss还是要重新归一下，最后logloss = 0.43512，跟前面的结果基本一致。</p>
<p>　　虽然最后两项的权重已经很接近于0，但看着还是很不爽，何不把很小的权重截成严格的0，就像生死符那样爽。对老司机来说都不是问题，上FTRL（为什么又是我）就好了嘛。</p>
<h2 id="u7B2C_u516B_u5F0F_uFF1A_u622A_u6743_u9053_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01"><a href="#u7B2C_u516B_u5F0F_uFF1A_u622A_u6743_u9053_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01_uFF01" class="headerlink" title="第八式：截权道！！！！！！！！"></a>第八式：截权道！！！！！！！！</h2><p>　　FTRL的稀疏性就是专职干这的，将w的adagrad改成FTRL，代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, math</span><br><span class="line"></span><br><span class="line">classNum = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">modelNum = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">alfa = float(sys.argv[<span class="number">3</span>])</span><br><span class="line">beta = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">l1 = float(sys.argv[<span class="number">4</span>])</span><br><span class="line">l2 = float(sys.argv[<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"classNum="</span> + str(classNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"modelNum="</span> + str(modelNum)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"alfa="</span> + str(alfa)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"l1="</span> + str(l1)</span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, <span class="string">"l2="</span> + str(l2)</span><br><span class="line"></span><br><span class="line">w = [<span class="number">1.0</span>] * modelNum</span><br><span class="line">nw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">zw = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">A = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">nA = [<span class="number">0.0</span>] * modelNum</span><br><span class="line">B = <span class="number">0.0</span></span><br><span class="line">nB = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(w)):</span><br><span class="line">    w[i] /= modelNum</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> &gt;&gt; sys.stderr, w</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    seg = line.rstrip().split(<span class="string">" "</span>)</span><br><span class="line">    label = int(seg[<span class="number">0</span>])</span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> seg[<span class="number">1</span>:]:</span><br><span class="line">        seg2 = tmp.split(<span class="string">","</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seg2)):</span><br><span class="line">            seg2[i] = float(seg2[i])</span><br><span class="line">        x.append(seg2)</span><br><span class="line">    pre = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    avgp = [<span class="number">0.0</span>] * classNum</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(classNum):</span><br><span class="line">            pre[k] += w[m]*x[m][k]</span><br><span class="line">            avgp[k] += x[m][k]/modelNum</span><br><span class="line">    wsum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(modelNum):</span><br><span class="line">        p = pre[label]</span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == p:</span><br><span class="line">            p = avgp[label]</span><br><span class="line">        gw = -x[m][label]/p - A[m] + B</span><br><span class="line">        sw = (math.sqrt(nw[m]+gw*gw)-math.sqrt(nw[m]))/alfa</span><br><span class="line">        zw[m] += gw - sw * w[m]</span><br><span class="line">        nw[m] += gw * gw</span><br><span class="line">        <span class="keyword">if</span> abs(zw[m]) &lt; l1:</span><br><span class="line">            w[m] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sgnz = <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> zw[m] &lt; <span class="number">0</span>:</span><br><span class="line">                sgnz = -<span class="number">1</span></span><br><span class="line">            w[m] = (sgnz*l1 - zw[m])/((beta + math.sqrt(nw[m]))/alfa + l2)</span><br><span class="line">        wsum += w[m]</span><br><span class="line">        gA = -w[m]</span><br><span class="line">        nA[m] += gA * gA</span><br><span class="line">        lrA = alfa/(beta+math.sqrt(nA[m]))</span><br><span class="line">        A[m] += lrA * gA</span><br><span class="line">        <span class="keyword">if</span> A[m] &lt; <span class="number">0</span>:</span><br><span class="line">            A[m] = <span class="number">0</span></span><br><span class="line">    gB = wsum - <span class="number">1</span></span><br><span class="line">    nB += gB * gB</span><br><span class="line">    lrB = alfa/(beta+math.sqrt(nB))</span><br><span class="line">    B += lrB * gB</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> w:</span><br><span class="line">    <span class="keyword">print</span> x</span><br></pre></td></tr></table></figure>
<p>　　注意这里L1正则要设的很大才行，比如100，效果如下：</p>
<pre><code>0.117563109491  gender_cut_ftrl8000_val
0.0734000016105 gender_cut_ftrl_val
0.0378296537773 gender_nn_1005_val
0.234723810866  gender_cut_ftrl2000_val
0.461201320728  gender_cut_ftrl1000_val
0.0747847726281 gender_newFM_val
0               gender_xxFM_val
0               gender_result_fm
</code></pre><p>　　哈哈，最后两项终于被彻底干掉，最后的logloss = 0.435117，依然保持的很好。</p>
<p>　　以上所有方法，在gender的7分类上也做了实验，同样有效，懒得再贴结果。<br><br><br>　　故事终于讲完了，Ensembling met Lagrange, and they lived happily ever after.<br><br><br><br><br>上一篇转到微信公众号时加了段作者简介：<br><small>本文作者：张卫，8年多码农生涯，先后在腾讯、搜狗混过些时日，目前在小米负责广告算法。无甚特别，唯好数学物理，正所谓推公式无敌手，推妹子无得手的中二汉子。</small></p>
<p>结果被人吐槽成了征婚帖，好吧，这次改一改：</p>
<p><small>本文作者：张卫，8年多码农生涯，先后在腾讯、搜狗混过些时日，目前在小米负责广告算法。曾经年少轻狂，颇好武侠旧学，晨舞葬诗魂的冷月刀，暮弄渡鹤影的寒塘剑；而今终日代码公式为伍，间或寄情文字，重举旧时觞。</small></p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/11/22/ensembling_lagrange/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/10/16/fm_ftrl_softmax/">FM, FTRL, Softmax</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-10-16T05:52:45.000Z">Oct 16 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="FM_2C_FTRL_2C_Softmax"><a href="#FM_2C_FTRL_2C_Softmax" class="headerlink" title="FM, FTRL, Softmax"></a>FM, FTRL, Softmax</h1><h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>　　最近公司内部举办了一届数据挖掘大赛，题目是根据用户的一些属性和行为数据来预测性别和年龄区间，属于一个二分类问题（性别预测男女）和一个多分类问题（年龄分为7个区间），评判标准为logloss。共有五六十支队伍提交，我们组的三名小伙伴最终取得第三名的好成绩，跟前两名只有千分之一二的差距。</p>
<p>　　赛后总结，发现前6名全部使用了DNN模型，而我们团队比较特别的是，不只使用了DNN，还有FM，最终方案是六七个DNN模型和一个FM模型的ensembling。</p>
<p>　　其实比赛刚开始，他们使用的是XGBoost，因为XGBoost的名头实在太响。但这次比赛的数据量规模较大，训练样本数达到千万，XGBoost跑起来异常的慢，一个模型要跑一两天。于是我把几个月前写的FM工具给他们用，效果非常好，二分类只需十几分钟，多分类也就半个多小时，logloss和XGBoost基本持平，甚至更低。最终他们抛弃了XGBoost，使用FM在快速验证特征和模型融合方面都起到了很好的作用。此外，我们组另外两名实习生仅使用此FM工具就取得了第七名的成绩。</p>
<p>　　最初写此FM代码时正值alphaGo完虐人类，因此随手给这个工具起了个名字叫alphaFM，今天我就来分享一下这个工具是如何实现的。</p>
<h2 id="alphaFM_u4ECB_u7ECD"><a href="#alphaFM_u4ECB_u7ECD" class="headerlink" title="alphaFM介绍"></a>alphaFM介绍</h2><p>　　代码地址在：<br>　　<a href="https://github.com/CastellanZhang/alphaFM" target="_blank" rel="external">https://github.com/CastellanZhang/alphaFM</a><br>　　<a href="https://github.com/CastellanZhang/alphaFM_softmax" target="_blank" rel="external">https://github.com/CastellanZhang/alphaFM_softmax</a></p>
<p>　　alphaFM是Factorization Machines的一个单机多线程版本实现，用于解决二分类问题，比如CTR预估，优化算法采用了FTRL。我其实把sgd和adagrad的方法也实现了，但最终发现还是FTRL的效果最好。</p>
<p>　　实现alphaFM的初衷是解决大规模数据的FM训练，在我们真实的业务数据中，训练样本数常常是千万到亿级别，特征维度是百万到千万级别甚至上亿，这样规模的数据完全加载到内存训练已经不太现实，甚至下载到本地硬盘都很困难，一般都是经过spark生成样本直接存储在hdfs上。</p>
<p>　　alphaFM用于解决这样的问题特别适合，一边从hdfs下载，一边计算，一个典型的使用方法是这样：</p>
<p>　　训练：10个线程计算，factorization的维度是8，最后得到模型文件fm_model.txt</p>
<p>　　<code>hadoop fs -cat train_data_hdfs_path | ./fm_train -core 10 -dim 1,1,8 -m fm_model.txt</code></p>
<p>　　测试：10个线程计算，factorization的维度是8，加载模型文件fm_model.txt，最后输出预测结果文件fm_pre.txt</p>
<p>　　<code>hadoop fs -cat test_data_hdfs_path | ./fm_predict -core 10 -dim 8 -m fm_model.txt -out fm_pre.txt</code></p>
<p>　　当然，如果样本文件不大，也可以先下载到本地，然后再运行alphaFM。</p>
<p>　　由于采用了FTRL，调好参数后，训练样本只需过一遍即可收敛，无需多次迭代，因此alphaFM读取训练样本采用了管道的方式，这样的好处除了节省内存，还可以通过管道对输入数据做各种中间过程的转换，比如采样、格式变换等，无需重新生成训练样本，方便灵活做实验。</p>
<p>　　alphaFM还支持加载上次的模型，继续在新数据上训练，理论上可以一直这样增量式进行下去。</p>
<p>　　FTRL的好处之一是可以得到稀疏解，在LR上非常有效，但对于FM，模型参数v是个向量，对于每一个特征，必须w为0且v的每一维都为0才算稀疏解， 但这通常很难满足，所以加了一个force_v_sparse的参数，在训练过程中，每当w变成0时，就强制将对应的v变成0向量。这样就可以得到很好的稀疏效果，且在我的实验中发现最终对test样本的logloss没有什么影响。</p>
<p>　　当将dim参数设置为1,1,0时，alphaFM就退化成标准的LR的FTRL训练工具。不禁想起我们最早的LR的FTRL代码还是勇保同学写的，我现在的代码基本上还是沿用了当初的多线程思路，感慨一下。</p>
<p>　　alphaFM能够处理的特征维度取决于内存大小，训练样本基本不占内存，理论上可以处理任意多的数量。后续可以考虑基于ps框架把alphaFM改造成分布式版本，这样就可以支持更大的特征维度。</p>
<p>　　alphaFM_softmax是alphaFM的多分类版本。两个工具的具体使用方法和参数说明见代码的readme，这里不再详述。</p>
<p>　　接下来请各位打起精神，我们来推一推公式。诗云，万丈高楼平地起，牛不牛逼靠地基。公式就是算法工具的地基，公式整明白了，像我们这种”精通”C++的（谁简历里不是呢:-P），实现就是分分钟的事（装B中，勿扰：-）。</p>
<h2 id="u4E8C_u5206_u7C7B_u95EE_u9898"><a href="#u4E8C_u5206_u7C7B_u95EE_u9898" class="headerlink" title="二分类问题"></a>二分类问题</h2><p>　　对于二分类，最常见的模型是LR，搭配FTRL优化算法。LR的输出会用到sigmoid函数，定义为：<br>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}<br>$$<br>　　LR预测输入$x$是正样本的概率：<br>$$<br>P(y=1|x,w)=\frac{1}{1+e^{-w^Tx}}=\sigma(w^Tx)<br>$$<br>　　可以看到，$\sigma$函数的参数部分 $w^Tx$ 是一个线性函数，这也就是LR被称作线性模型的原因，模型参数只有一个$w$向量，相对简单。如果我们把这部分弄复杂呢？比如这样：<br>$$<br>\hat{y}(x|\Theta):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\langle v_i,v_j\rangle x_ix_j\\<br>=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^nx_ix_j\sum_{f=1}^kv_{i,f}v_{j,f}\\<br>=w_0+\sum_{i=1}^nw_ix_i+\frac{1}{2}\sum_{f=1}^k\left(\left(\sum_{i=1}^nv_{i,f}x_i\right)^2-\sum_{i=1}^nv_{i,f}^2x_i^2\right)<br>$$<br>　　其中，$x\in R^n$，$w_0\in R$，$w\in R^n$，$V\in R^{n\times k}$，这其实就是一个2阶FM，模型参数 $\Theta=\{w_0,w_1,…,w_n,v_{1,1},…,v_{n,k}\}$ 。如果直接将 $\hat{y}(x|\Theta)$ 做输出，采用平方损失函数便可解决回归问题。而对于二分类问题，外面套一个sigmoid函数即可：<br>$$<br>P(y=1|x,\Theta)=\frac{1}{1+e^{-\hat{y}(x|\Theta)}}<br>$$<br>　　对于$y\in \{-1,1\}$，可统一成形式：<br>$$<br>P(y|x,\Theta)=\frac{1}{1+e^{-y\hat{y}(x|\Theta)}}=\sigma(y\hat{y}(x|\Theta))<br>$$<br>　　模型参数估计采用最大似然的方法，对于训练数据$S$，最优化问题为：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(x,y)\in S}P(y|x,\Theta)=\mathop{\arg\min}_{\Theta}\sum_{(x,y)\in S}-\ln P(y|x,\Theta)<br>$$<br>　　即样本 $(x,y)$ 的损失函数为：<br>$$<br>l(\Theta|x,y)=-\ln P(y|x,\Theta)=-\ln \sigma(y\hat{y}(x|\Theta))<br>$$<br>　　此损失函数对 $\hat{y}$ 求偏导会得到一个优雅简单的形式：<br>$$<br>\frac{\partial l}{\partial\hat{y}}=y(\sigma(y\hat{y})-1)<br>$$<br>　　再配合上 $\hat{y}$ 对模型参数的偏导：<br>$$<br>\frac{\partial\hat{y}}{\partial\theta}=<br>\begin{cases}<br>  1,   &amp; if\,\,\theta\,\,is\,\,w_0 \\<br>  x_i, &amp; if\,\,\theta\,\,is\,\,w_i \\<br>  x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 &amp; if\,\,\theta\,\,is\,\,v_{i,f} \\<br>\end{cases}<br>$$<br>　　便可得到损失函数 $l$ 对所有模型参数的偏导，即：<br>$$<br>g_0^w=\frac{\partial l}{\partial w_0}=y(\sigma(y\hat{y})-1)\\<br>g_i^w=\frac{\partial l}{\partial w_i}=y(\sigma(y\hat{y})-1)x_i\\<br>g_{i,f}^v=\frac{\partial l}{\partial v_{i,f}}=y(\sigma(y\hat{y})-1)(x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2)<br>$$</p>
<p>此时，我们能够很自然的想到用SGD的方法来求解模型参数，但我这里采用了更加高效的FTRL优化算法。</p>
<p>　　让我们来简单回顾一下FTRL，Google在2013年放出这个优化方法，迅速火遍大江南北，原始论文里只是用来解决LR问题，论文截图如下：</p>
<p><img src="/img/ftrl.jpg" alt=""></p>
<p>　　但其实FTRL是一个online learning的框架，能解决的问题绝不仅仅是LR，已经成了一个通用的优化算子，比如TensorFlow的optimizer中都包含了FTRL。我们只要把截图中的伪代码修改，$p_t$的计算改为 $\hat{y}(x|\Theta)$，对于每一轮的特征向量$x$的每一维非0特征$x_i$，都要相应的更新模型参数$w_0,w_i,v_{i,1},…,v_{i,k}$，更新公式不变和截图一致，梯度$g$的计算即为损失函数对每个参数的偏导，前面已经给出。$\sigma,z,n$的更新公式不变。伪代码如下：</p>
<hr>
<p>Algorithm: alphaFM</p>
<hr>
<p>$Input:paramters\,\alpha^w,\alpha^v,\beta^w,\beta^v,\lambda_1^w,\lambda_1^v,\lambda_2^w,\lambda_2^v,\sigma$<br>$Init:w_0=0;n_0^w=0;z_0^w=0;$<br>$Init:\forall i,\forall f,w_i=0;n_i^w=0;z_i^w=0;v_{i,f}\sim N(0,\sigma);n_{i,f}^v=0;z_{i,f}^v=0;$<br>$for\,t=1\,to\,T,do$<br>$\qquad Receive\,feature\,vector\,x\,and\,let\,I=\{i|x_i\neq0\}$<br>$$<br>w_0=<br>\begin{cases}<br>  0   &amp; if\,\,|z_0^w|\le\lambda_1^w \\<br>  -\left(\frac{\beta^w+\sqrt{n_0^w}}{\alpha^w}+\lambda_2^w\right)^{-1}(z_0^w-sgn(z_0^w)\lambda_1^w) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad for\,i\in I,compute$<br>$$<br>w_i=<br>\begin{cases}<br>  0   &amp; if\,\,|z_i^w|\le\lambda_1^w \\<br>  -\left(\frac{\beta^w+\sqrt{n_i^w}}{\alpha^w}+\lambda_2^w\right)^{-1}(z_i^w-sgn(z_i^w)\lambda_1^w) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad \qquad for\,f=1\,to\,k,compute$<br>$$<br>v_{i,f}=<br>\begin{cases}<br>  0   &amp; if\,\,|z_{i,f}^v|\le\lambda_1^v \\<br>  -\left(\frac{\beta^v+\sqrt{n_{i,f}^v}}{\alpha^v}+\lambda_2^v\right)^{-1}(z_{i,f}^v-sgn(z_{i,f}^v)\lambda_1^v) &amp; otherwise. \\<br>\end{cases}<br>$$<br>$\qquad \qquad end\,for$<br>$\qquad end\,for$<br>$\qquad Compute\,\hat{y}(x|\Theta)$<br>$\qquad Observe\,label\,y\in\{-1,1\}$<br>$\qquad compute\,g_0^w$<br>$\qquad \sigma_0^w=\frac{1}{\alpha^w}(\sqrt{n_0^w+(g_0^w)^2}-\sqrt{n_0^w})$<br>$\qquad z_0^w\leftarrow z_0^w+g_0^w-\sigma_0^ww_0$<br>$\qquad n_0^w\leftarrow n_0^w+(g_0^w)^2$<br>$\qquad for\,i\in I,do$<br>$\qquad \qquad compute\,g_i^w$<br>$\qquad \qquad \sigma_i^w=\frac{1}{\alpha^w}(\sqrt{n_i^w+(g_i^w)^2}-\sqrt{n_i^w})$<br>$\qquad \qquad z_i^w\leftarrow z_i^w+g_i^w-\sigma_i^ww_i$<br>$\qquad \qquad n_i^w\leftarrow n_i^w+(g_i^w)^2$<br>$\qquad \qquad for\,f=1\,to\,k,do$<br>$\qquad \qquad \qquad compute\,g_{i,f}^v$<br>$\qquad \qquad \qquad \sigma_{i,f}^v=\frac{1}{\alpha^v}(\sqrt{n_{i,f}^v+(g_{i,f}^v)^2}-\sqrt{n_{i,f}^v})$<br>$\qquad \qquad \qquad z_{i,f}^v\leftarrow z_{i,f}^v+g_{i,f}^v-\sigma_{i,f}^vv_{i,f}$<br>$\qquad \qquad \qquad n_{i,f}^v\leftarrow n_{i,f}^v+(g_{i,f}^v)^2$<br>$\qquad \qquad end\,for$<br>$\qquad end\,for$<br>$end\,for$    </p>
<hr>
<h2 id="u591A_u5206_u7C7B_u95EE_u9898"><a href="#u591A_u5206_u7C7B_u95EE_u9898" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>　　Softmax模型是LR在多分类上的推广，具体介绍戳<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="external">这里</a>。大致就是如果有$c$个类别，则模型参数为$c$个向量：$\Theta=\{w_1,w_2,…,w_c\}$，其中任意$w_i\in R^n$。</p>
<p>　　样本$x$属于类别$i$的概率：<br>$$<br>P(y=i|x,\Theta)=\frac{e^{w_i^Tx}}{\sum_{j=1}^ce^{w_j^Tx}}<br>$$<br>　　FM解决多分类的方法同样是将线性部分$w^Tx$替换成复杂的 $\hat{y}(x|\Theta)$，不过不再是一个 $\hat{y}$，而是每一类别对应一个，共$c$个：$\hat{y}_1(x|\Theta),…,\hat{y}_c(x|\Theta)$</p>
<p>　　样本$x$属于类别$i$的概率也变成：<br>$$<br>P(y=i|x,\Theta)=\frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}<br>$$<br>　　模型参数一共$c$组， $\Theta=\{\Theta_1,…,\Theta_c\}$，其中类别$i$对应一组参数 $\Theta_i=\{w_0^i,w_1^i,…,w_n^i,v_{1,1}^i,…,v_{n,k}^i\}$</p>
<p>　　我们定义一个示性函数 $1\{\cdot\}$，大括号中表达式为真则值为1，表达式为假则值为0。这样就可以写出最优化问题：<br>$$<br>\mathop{\arg\max}_{\Theta}\prod_{(x,y)\in S}\prod_{i=1}^cP(y=i|x,\Theta)^{1\{y=i\}}\\<br>=\mathop{\arg\min}_{\Theta}-\sum_{(x,y)\in S}\sum_{i=1}^c1\{y=i\}\ln P(y=i|x,\Theta)<br>$$<br>　　每条样本 $(x,y)$ 的损失函数：<br>$$<br>l(\Theta|x,y)=-\sum_{i=1}^c1\{y=i\}\ln P(y=i|x,\Theta)\\<br>=-\sum_{i=1}^c1\{y=i\}\ln \frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}\\<br>=\sum_{i=1}^c1\{y=i\}(\ln\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}-\hat{y}_i(x|\Theta))\\<br>=\ln\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}-\sum_{i=1}^c1\{y=i\}\hat{y}_i(x|\Theta)<br>$$<br>　　梯度：<br>$$<br>\nabla_{\Theta_i}l(\Theta|x,y)=\frac{\partial l}{\partial\hat{y}_i}\nabla_{\Theta_i}\hat{y}_i(x|\Theta)<br>$$<br>　　而<br>$$<br>\frac{\partial l}{\partial\hat{y}_i}=\frac{e^{\hat{y}_i(x|\Theta)}}{\sum_{j=1}^ce^{\hat{y}_j(x|\Theta)}}-1\{y=i\}\\<br>=P(y=i|x,\Theta)-1\{y=i\}<br>$$<br>　　所以有<br>$$<br>\nabla_{\Theta_i}l(\Theta|x,y)=(P(y=i|x,\Theta)-1\{y=i\})\nabla_{\Theta_i}\hat{y}_i(x|\Theta)<br>$$<br>$\nabla_{\Theta_i}\hat{y}_i(x|\Theta)$ 即求 $\hat{y}_i$ 对 $\Theta_i$ 中所有参数 $\{w_0^i,w_1^i,…,w_n^i,v_{1,1}^i,…,v_{n,k}^i\}$ 的偏导，这在二分类中我们已经给出。</p>
<p>　　最后，仍然是套用FTRL的框架，只是每条样本更新的参数更多，不再细说，详见代码。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/10/16/fm_ftrl_softmax/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/02/05/cf_als/">CF的ALS算法推导</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-02-05T03:47:45.000Z">Feb 5 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <p>　　在上一篇中介绍了矩阵微分，现在就来牛刀小试一下。早些时候子龙问过我 Collaborative Filtering for Implicit Feedback Datasets这篇论文里的公式推导，在这里重新解一遍。<br>　　论文里给出的目标函数为：<br>$$<br>\min_{x_*,y_*}\left\{\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda(\sum_u||x_u||^2+\sum_i||y_i||^2)\right\}\,\,\,\,(1)<br>$$<br>　　模型的来历不是今天的重点，简单描述一下各个变量的含义，其中$u$代表user，$i$代表item，$x_u\in R^f$，$y_i\in R^f$，未出现的变量 $r_{ui}$ 表示user对item的“消费”度量，<br>$$<br>p_{ui}=<br>\begin{cases}<br>  1 &amp; r_{ui}&gt;0 \\<br>  0 &amp; r_{ui}=0 \\<br>\end{cases}<br>$$<br>表征user是否对item有偏好，$c_{ui}=1+\alpha r_{ui}$ 表征$p_{ui}$ 的置信度。<br>　　假设user的数目为$m$，item的数目为$n$，则目标函数包含的项大约有$m\cdot n$，通常数量非常巨大，一些常见的优化算法比如SGD不再适合，论文中采用了ALS（alternating-least-squares）算法，这是一种迭代的方法，第一步先固定所有的$y_i$，求解最优的$x_u$；第二步固定所有的$x_u$，求解最优的$y_i$，依次迭代。<br>　　我们先看第一步，当固定所有$y_i$后，(1)式转化为<br>$$<br>\min_{x_*}\left\{\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda\sum_u||x_u||^2\right\}\\<br>=\sum_u\min_{x_u}\left\{\sum_{i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda||x_u||^2\right\}\\<br>=\sum_u\min_{x_u}L_u(x_u)<br>$$<br>问题简化为求每个$L_u(x_u)$ 函数的最小值。<br>　　对每个$u$，我们定义一个$n\times n$的对角矩阵$C^u$，其中$C_{ii}^u=c_{ui}$，定义向量$p(u)\in R^n$包含所有的$p_{ui}$。激动人心的时刻来到了，我们开始推导函数$L_u(x_u)$ 对$x_u$的微分<br>$$<br>L_u(x_u)=\sum_{i}c_{ui}(p_{ui}-x_u^Ty_i)^2+\lambda||x_u||^2\\<br>=<br>\sum_{i}c_{ui}(y_i^Tx_u-p_{ui})^2+\lambda x_u^Tx_u\\<br>dL_u=\sum_{i}2c_{ui}(x_u^Ty_i-p_{ui})y_i^Tdx_u+2\lambda x_u^Tdx_u\\<br>=2\left(\sum_{i}c_{ui}x_u^Ty_iy_i^T-\sum_{i}c_{ui}p_{ui}y_i^T+\lambda x_u^T\right)dx_u\\<br>$$<br>　　$L_u(x_u)$ 取极值，须$dL_u=0$，有<br>$$<br>\sum_{i}c_{ui}x_u^Ty_iy_i^T-\sum_{i}c_{ui}p_{ui}y_i^T+\lambda x_u^T=0\\<br>\Rightarrow \sum_{i}c_{ui}y_iy_i^Tx_u+\lambda Ix_u=\sum_{i}c_{ui}p_{ui}y_i\\<br>\Rightarrow \left(\sum_{i}c_{ui}y_iy_i^T+\lambda I\right)x_u=\sum_{i}c_{ui}p_{ui}y_i\,\,\,\,(2)<br>$$<br>　　按论文中的符号定义，设$Y^T=(y_1,\cdots,y_n)$，有<br>$$<br>\sum_{i}c_{ui}y_iy_i^T=(c_{u1}y_1,\cdots,c_{un}y_n)<br>\left(\begin{array}{c}<br>  y_1^T \\<br>  \vdots \\<br>  y_n^T \\<br>\end{array}\right)\\<br>=(y_1,\cdots,y_n)<br>\left(\begin{array}{ccc}<br>  c_{u1} &amp;  &amp;  \\<br>  &amp; \ddots &amp;  \\<br>  &amp;  &amp;  c_{un} \\<br>\end{array}\right)<br>\left(\begin{array}{c}<br>  y_1^T \\<br>  \vdots \\<br>  y_n^T \\<br>\end{array}\right)\\<br>=Y^TC^uY<br>$$<br>　　类似地，有 $\sum_{i}c_{ui}p_{ui}y_i=Y^TC^up(u)$，因此<br>$$<br>(2)\Rightarrow (Y^TC^uY+\lambda I)x_u=Y^TC^up(u)\\<br>\Rightarrow x_u=(Y^TC^uY+\lambda I)^{-1}Y^TC^up(u)<br>$$<br>正是论文中的结果。对于固定$x_u$求$y_i$完全类似，不再详述。</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/02/05/cf_als/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  
    <article class="post">
  
  
    <div class="post-content-index">
  
      
        <header>
          <div class="icon"></div>
            
    
      <h1 class="title transition"><a href="/2016/02/02/matrix_differential/">Matrix Differential</a></h1>
    
  
          <ul>
            <li>
              <span class="heading-span">Posted on: </span>
              <time datetime="2016-02-02T12:45:11.000Z">Feb 2 2016</time>
            </li>
            
              <li>
                <span class="heading-span">By: </span>

                
                  <a href="/">CastellanZhang</a>
                

              </li>
            
            <li>
              <span class="heading-span">With: </span>
              
          </ul>
        </header>
      
      <div class="entry">
        
          
            <h1 id="u77E9_u9635_u5FAE_u5206_28Matrix_Differential_29"><a href="#u77E9_u9635_u5FAE_u5206_28Matrix_Differential_29" class="headerlink" title="矩阵微分(Matrix Differential)"></a>矩阵微分(Matrix Differential)</h1><p>　　在最优化问题中，经常涉及到导数或梯度的计算，比如 $\nabla(x’Ax)$，虽然自己最终也能推导出来等于 $(A+A’)x$，但总感觉在这块缺少系统的知识，有时还会想向量函数的导数又如何计算，矩阵函数呢？在网上搜了很久，似乎这块的知识点一直比较混乱，wikipedia的Matrix calculus词条也写得不好，甚至有人质疑。大约一年前又去中关村图书大厦遍寻矩阵相关的书籍，只找到清华出的一本《矩阵分析与应用》涉及到矩阵微分，手机拍了几十张照片回来细读仍不甚满意。无意间却从网上找到了这本书：Matrix Differential Calculus with Applications in Statistics and Econometrics，正是我想要的！（其实清华那本和wikipedia都提到了这本书，只是当时没注意。）书甚厚，博大精深，我把感兴趣的部分边看边做笔记，唯纸薄字陋难存久，便想着不如写成blog，可惜本是懒惰之人一拖便是一年，直到最近失意百无聊赖，终于完成。文章憎命达，诚如是。</p>
<h2 id="1-__u57FA_u7840_u6982_u5FF5"><a href="#1-__u57FA_u7840_u6982_u5FF5" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h2><p>　　向量$x$的范数(norm)定义为：<br>$$||x||=(x’x)^{1/2}$$</p>
<p>　　假设$a$为$n \times 1$向量，$A$为$n \times n$矩阵，$B$为$n \times m$矩阵，则$a’x$称为向量$x$的线性型，$x’Ax$称为$x$的二次型，$x’By$称为$x$和$y$的双线性型。<br>　　不失一般性，我们假定$A$为对称阵，因为我们总可以用 $(A+A’)/2$ 来代替，推导如下：<br>$$x’(A+A’)x=x’Ax+x’A’x=x’Ax+(x’A’x)’=x’Ax+x’Ax=2x’Ax$$<br>　　我们说，$A$是正定的，如果对所有$x\ne0$，有$x’Ax&gt;0$；$A$是半正定的，如果对所有$x$，有$x’Ax\ge0$。负定的定义类似，不再赘述。<br>　　易证 $BB’$ 和 $B’B$ 都是半正定的，因为 $x’B’Bx=(Bx)’Bx\ge0$  </p>
<p>　　方阵的迹(trace)定义为：</p>
<p>$$trA=\sum_{i=1}^na_{ii}$$</p>
<p>　　矩阵的范数定义为：</p>
<p>$$||A||=(trA’A)^{1/2}$$</p>
<p>　　矩阵的直积：<br>　　$A$为$m\times n$矩阵，$B$为$p\times q$矩阵，则$A\otimes B$为$mp\times nq$的矩阵<br>$$<br>A\otimes B=<br>\left(\begin{array}{ccc}<br>  a_{11}B &amp; \cdots &amp; a_{1n}B \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  a_{m1}B &amp; \cdots &amp; a_{mn}B \\<br>\end{array}\right)<br>$$</p>
<p>　　VEC算子：<br>　　设$A$为$m\times n$矩阵，$a_i$为其第$i$列，则$vec\,A$为$mn\times 1$向量<br>$$<br>vec\,A=<br>\left(\begin{array}{c}<br>  a_1 \\<br>  a_2 \\<br>  \vdots \\<br>  a_n<br>\end{array}\right)<br>$$<br>　　几个相关公式:<br>$$vec\,a’=vec\,a=a$$<br>$$vec\,ab’=b\otimes a$$<br>$$(vec\,A)’vec\,B=tr\,A’B$$<br>$$vec\,ABC=(C’\otimes A)vec\,B$$<br>$$vec\,AB=(B’\otimes I_m)vec\,A=(B’\otimes A)vec\,I_n=(I_q\otimes A)vec\,B,\,\,\,\,A:m\times n,B:n\times q$$</p>
<h2 id="2-__u5FAE_u5206_28differential_29_u7684_u5B9A_u4E49"><a href="#2-__u5FAE_u5206_28differential_29_u7684_u5B9A_u4E49" class="headerlink" title="2. 微分(differential)的定义"></a>2. 微分(differential)的定义</h2><h3 id="2-1__u6807_u91CF_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-1__u6807_u91CF_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.1 标量函数的微分"></a>2.1 标量函数的微分</h3><p>　　对于一维的情况，$\phi(x):R\rightarrow R$<br>$$\phi(c+u)=\phi(c)+u\phi’(c)+r_c(u)$$<br>$$\lim_{u\rightarrow 0}\frac{r_c(u)}{u}=0$$<br>　　定义 $\phi$ 在$c$点基于增量$u$的一阶微分为<br>$$d\phi(c;u)=u\phi’(c)$$</p>
<h3 id="2-2__u5411_u91CF_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-2__u5411_u91CF_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.2 向量函数的微分"></a>2.2 向量函数的微分</h3><p>　　设函数$f:S\rightarrow R^m$，$S\subset R^n$，$c$是$S$的一个内点(interior point)，$B(c;r)$ 是$S$中$c$点的一个邻域(n-ball)，$u$是$R^n$中的一点，满足 $||u||&lt;r$，因此有$c+u\in B(c;r)$，如果存在一个$m\times n$实矩阵$A$，满足<br>$$f(c+u)=f(c)+A(c)u+r_c(u)$$<br>对于所有的$u\in R^n$，$||u||&lt;r$，且<br>$$\lim_{u\rightarrow0}\frac{r_c(u)}{||u||}=0$$<br>这样，函数$f$就被称为在$c$点可微。矩阵$A(c)$ 称为$f$在$c$点的一阶导数。$m\times 1$向量<br>$$df(c;u)=A(c)u$$<br>称为$f$在$c$点的一阶微分（基于增量$u$）。</p>
<h3 id="2-3__u5411_u91CF_u51FD_u6570_u7684_u504F_u5BFC_u6570"><a href="#2-3__u5411_u91CF_u51FD_u6570_u7684_u504F_u5BFC_u6570" class="headerlink" title="2.3 向量函数的偏导数"></a>2.3 向量函数的偏导数</h3><p>　　令$f=(f_1,f_2,\cdots,f_m)$，$t\in R$，如果极限<br>$$\lim_{t\rightarrow0}\frac{f_i(c+te_j)-f_i(c)}{t}$$<br>存在，则称之为$f_i$在$c$点的第$j$个偏导数，记为$D_jf_i(c)$，或者 $[\partial f_i(x)/\partial x_j]_{x=c}$ 或者 $\partial f_i(c)/\partial x_j$。</p>
<p>　　如果$f$在$c$点可微，则所有的偏导数$D_jf_i(c)$ 存在。反过来不一定成立。</p>
<p>　　如果$f$在$c$点可微，那么存在矩阵$A(c)$，<br>$$f(c+u)=f(c)+A(c)u+r_c(u)$$<br>这里忽略了一些限制条件，详见前面向量的微分。<br>　　$A(c)$ 的每一项$a_{ij}(c)$ 其实就是相应的偏导数$D_jf_i(c)$，即<br>$$A(c)=Df(c)$$<br>这里的$Df(c)$ 被称作雅可比矩阵(Jacobian matrix)，注意$A(c)$ 存在时$Df(c)$ 一定存在，反之未必。  </p>
<h3 id="2-4__u68AF_u5EA6_28gradient_29_u7684_u5B9A_u4E49"><a href="#2-4__u68AF_u5EA6_28gradient_29_u7684_u5B9A_u4E49" class="headerlink" title="2.4 梯度(gradient)的定义"></a>2.4 梯度(gradient)的定义</h3><p>　　$Df(c)$ 的转置称为$f$在$c$点的梯度，用 $\nabla f(c)$ 表示，即<br>$$\nabla f(c)=(Df(c))’$$<br>　　当向量函数$f:S\rightarrow R^m$ 退化成标量函数 $\phi:S\rightarrow R$ 时，雅可比矩阵退化成$1\times n$行向量$D\phi(c)$，梯度退化成$n\times 1$列向量 $\nabla\phi(c)$。</p>
<h3 id="2-5__u77E9_u9635_u51FD_u6570_u7684_u5FAE_u5206"><a href="#2-5__u77E9_u9635_u51FD_u6570_u7684_u5FAE_u5206" class="headerlink" title="2.5 矩阵函数的微分"></a>2.5 矩阵函数的微分</h3><p>　　现在终于轮到矩阵函数出场了，令 $F:S\rightarrow R^{m\times p}$，其中 $S\subset R^{n\times q}$。$C$是$S$的内点，且令 $B(C;r)=\{X:X\in R^{n\times q},||X-C||&lt;r\}$，这里 $||X||=(trX’X)^{1/2}$。<br>　　设点$U$是$R^{n\times q}$ 内一点，满足 $||U||&lt;r$，因此有 $C+U\in B(C;r)$。如果存在一个 $mp\times nq$ 的矩阵 $A$，有<br>$$vec\,F(C+U)=vec\,F(C)+A(C)vec\,U+vec\,R_C(U)$$<br>对于所有 $U\in R^{n\times q}$，$||U||&lt;r$，且<br>$$\lim_{U\rightarrow0}\frac{R_C(U)}{||U||}=0$$<br>那么，函数 $F$ 被称为在 $C$ 点可微。<br>　　$m\times p$ 矩阵 $dF(C;U)$ 由下式定义<br>$$vec\,dF(C;U)=A(C)vec\,U$$<br>被称作 $F$ 在 $C$ 点基于增量 $U$ 的一阶微分，$mp\times nq$ 矩阵 $A(C)$ 被称作 $F$ 在 $C$ 点的一阶导数。</p>
<p>　　可以看到，这里的做法就是将矩阵函数化为向量函数来处理。对于每个矩阵函数 $F$（自变量和函数值均为矩阵），<br>我们都可以构造一个对应的向量函数$f:vec\,S\rightarrow R^{mp}$（自变量和函数值均为向量）<br>$$f(vec\,X)=vec\,F(X)$$<br>易得<br>$$vec\,dF(C;U)=df(vec\,C;vec\,U)$$<br>我们定义 $F$ 在 $C$ 点的雅可比矩阵为<br>$$DF(C)=Df(vec\,C)$$<br>这是一个 $mp\times nq$ 的矩阵，其第 $ij$ 元素是$vec\,F$的第 $i$ 个分量对 $vec\,X$ 的第 $j$个分量在 $X=C$ 处的偏导数。</p>
<p>　　当 $F$ 在 $C$ 点可微，有 $DF(C)=A(C)$</p>
<p>　　设 $U$ 和 $V$ 是矩阵函数，$A$ 是矩阵常量，有<br>$$dA=0$$<br>$$d(\alpha U)=\alpha dU$$<br>$$d(U+V)=dU+dV$$<br>$$d(U-V)=dU-dV$$<br>$$d(UV)=(dU)V+UdV$$<br>$$dU’=(dU)’$$<br>$$d\,vec\,U=vec\,dU$$<br>$$d\,tr\,U=tr\,dU$$</p>
<h3 id="2-6__u94FE_u5F0F_u6CD5_u5219"><a href="#2-6__u94FE_u5F0F_u6CD5_u5219" class="headerlink" title="2.6 链式法则"></a>2.6 链式法则</h3><p>　　同最简单的标量自变量的标量函数一样，链式法则同样成立。<br>　　设函数$f:S\rightarrow R^m$，$S\subset R^n$，在 $c$ 点可微。$f(x)\in T$，设函数$g:T\rightarrow R^p$在 $b=f(c)$ 点可微。定义复合函数$h:S\rightarrow R^p$ 如下：<br>$$<br>h(x)=g(f(x))<br>$$<br>那么，$h$ 在 $c$ 点可微，并且<br>$$<br>Dh(c)=(Dg(b))(Df(c))<br>$$<br>$$<br>dh(c;u)=(Dg(b))(Df(c))u=(Dg(b))df(c;u)=dg(b;df(c;u))<br>$$<br>　　对矩阵函数类似，省略掉函数定义，直接给出公式：<br>$$<br>H(X)=G(F(X))<br>$$<br>$$<br>DH(C)=(DG(B))(DF(C))<br>$$<br>$$<br>dH(C;U)=dG(B;dF(C;U))<br>$$<br>　　我们还可以简写微分符号：<br>$$<br>y=g(t)<br>$$<br>$$<br>dy=dg(t;dt)<br>$$<br>$$<br>t=f(x)<br>$$<br>$$<br>y=g(f(x))\equiv h(x)<br>$$<br>$$<br>dy=dh(x;dx)=dg(f(x);df(x;dx))=dg(t;dt)<br>$$<br>　　我们甚至可以不用$y$直接用$g$本身来简写：<br>$$<br>dg=dg(t;dt)<br>$$<br>　　举例：<br>$$<br>y=\phi(x)=e^{x’x}\\<br>dy=de^{x’x}=e^{x’x}(dx’x)=e^{x’x}((dx)’x+x’dx)=(2e^{x’x}x’)dx<br>$$<br>$$<br>z=\phi(\beta)=(y-X\beta)’(y-X\beta)<br>$$<br>设$e=y-X\beta$，则<br>$$<br>dz=de’e=2e’de=2e’d(y-X\beta)\\<br>=-2e’Xd\beta=-2(y-X\beta)’Xd\beta<br>$$</p>
<h2 id="3-__u96C5_u53EF_u6BD4_u77E9_u9635_28Jacobian_matrix_29"><a href="#3-__u96C5_u53EF_u6BD4_u77E9_u9635_28Jacobian_matrix_29" class="headerlink" title="3. 雅可比矩阵(Jacobian matrix)"></a>3. 雅可比矩阵(Jacobian matrix)</h2><p>　　重新梳理一下雅克比矩阵的求法，基本思路如下：<br>　　给定一个矩阵函数$F(X)$<br>　　(1)计算$F(X)$ 的微分<br>　　(2)向量化得到$d\,vec\,F(X)=A(X)d\,vec\,X$<br>　　(3)得到雅克比矩阵$DF(X)=A(X)$  </p>
<p>　　我们再次明确一下自变量和函数的符号，见下表<br>$$<br>\begin{array}{cccc}<br>\hline<br>  &amp; Scalar   &amp; Vector   &amp; Matrix   \\<br>  &amp; variable &amp; variable &amp; variable \\<br>\hline<br>Scalar\,function &amp; \phi(\xi) &amp; \phi(x) &amp; \phi(X) \\<br>Vector\,function &amp; f(\xi) &amp; f(x) &amp; f(X) \\<br>Matrix\,function &amp; F(\xi) &amp; F(x) &amp; F(X) \\<br>\hline<br>\end{array}<br>$$</p>
<p>　　雅克比矩阵本质上就是要解决如何排列 $F(X)$ 的所有偏导数 $\partial f_{st}(X)/\partial x_{ij}$ 的问题。<br>　　先给出一些符号定义：<br>　　$\phi$为标量函数，$X=(x_{ij})$ 为 $n\times q$ 矩阵，$F=(f_{st})$ 为 $m\times p$ 矩阵<br>$$<br>\frac{\partial\phi(X)}{\partial X}=<br>\left(\begin{array}{ccc}<br>  \partial\phi/\partial x_{11} &amp; \cdots &amp; \partial\phi/\partial x_{1q} \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  \partial\phi/\partial x_{n1} &amp; \cdots &amp; \partial\phi/\partial x_{nq} \\<br>\end{array}\right)<br>$$</p>
<p>$$<br>\frac{\partial F(X)}{\partial X}=<br>\left(\begin{array}{ccc}<br>  \partial f_{11}/\partial X &amp; \cdots &amp; \partial f_{1p}/\partial X \\<br>  \vdots  &amp;        &amp; \vdots  \\<br>  \partial f_{m1}/\partial X &amp; \cdots &amp; \partial f_{mp}/\partial X \\<br>\end{array}\right)<br>$$</p>
<p>　　特别地，$\phi$为标量函数，$\partial\phi/\partial x$是列向量，$\partial\phi/\partial x’$ 是行向量；$m\times1$的向量函数$f(x)$，$x$为$n\times1$向量，可以有四种排列：$\partial f/\partial x’$（$m\times n$矩阵），$\partial f’/\partial x$（$n\times m$矩阵），$\partial f/\partial x$（$mn\times 1$矩阵），$\partial f’/\partial x’$（$1\times mn$矩阵）。<br>　　下面给出雅克比矩阵的符号定义：<br>$$<br>D\phi(x)=(D_1\phi(x),\dots,D_n(x))=\frac{\partial\phi(x)}{\partial x’}<br>$$<br>$$<br>Df(x)=\frac{\partial f(x)}{\partial x’}<br>$$<br>$$<br>DF(X)=\frac{\partial\,vec\,F(X)}{\partial(vec\,X)’}<br>$$<br>　　可以对比一下，$DF(X)$ 是 $mp\times nq$，而 $\partial F(X)/\partial X$ 是$mn\times pq$。<br>　　举几个例子，$F(X)=AXB$，则<br>$$dF(X)=A(dX)B$$<br>$$d\,vec\,F(X)=(B’\otimes A)d\,vec\,X$$<br>　　因此有<br>$$DF(X)=B’\otimes A$$<br>　　令 $\phi(x)=x’Ax$，则<br>$$<br>d\phi(x)=d(x’Ax)=(dx)’Ax+x’Adx\\<br>=((dx)’Ax)’+x’Ax=x’A’dx+x’Adx\\<br>=x’(A+A’)dx<br>$$<br>　　因此有<br>$$<br>D\phi(x)=x’(A+A’)<br>$$</p>

          
        
      </div>
      <footer>
        
          <div class="alignright">
            <a href="/2016/02/02/matrix_differential/#more" class="more-link">Continue Reading<i class="fa fa-long-arrow-right fa-1"></i></a>
          </div>
        
        <div class="clearfix"></div>
      </footer>
    </div>
</article>



  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>




   </div></div>
    <aside id="sidebar" class="alignright"><div class="padding">
	
	  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>
	
	  
<div class="widget recent-post">
  <h3 class="title">最新文章</h3>
  <ul class="entry">
    
      <li>
        <a href="/2017/10/21/ocpc_roi/">Paper Reading: OCPC, ROI</a>
      </li>
    
      <li>
        <a href="/2017/10/08/explore_exploit/">Paper Reading: Explore and Exploit</a>
      </li>
    
      <li>
        <a href="/2017/07/16/lambdafm/">lambdaFM</a>
      </li>
    
      <li>
        <a href="/2017/06/01/mlr_plm/">MLR, PLM</a>
      </li>
    
      <li>
        <a href="/2016/11/22/ensembling_lagrange/">Ensembling, Lagrange</a>
      </li>
    
      <li>
        <a href="/2016/10/16/fm_ftrl_softmax/">FM, FTRL, Softmax</a>
      </li>
    
      <li>
        <a href="/2016/02/05/cf_als/">CF的ALS算法推导</a>
      </li>
    
      <li>
        <a href="/2016/02/02/matrix_differential/">Matrix Differential</a>
      </li>
    
  </ul>
</div>


	
	  
	
	  
	
</div></aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="padding">
	<div class="alignleft">
	  
	  &copy; 2017 CastellanZhang
	  
	  Powerd by <a href="http://hexo.io/" target="_blank">hexo</a>
	  and Theme by <a href="https://github.com/halfer53/metro-light" target="_blank">metro-light</a>
	</div>

	<div class="alignright">
		
		
		
		
		
		
		
	</div>

	<div class="clearfix"></div>
</div>

<div class="scroll-top"><i class="fa fa-arrow-circle-up"></i></div></footer>
  


<script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/3.0.4/jquery.imagesloaded.js"></script>
<script src="/js/gallery.js"></script>



<script type="text/javascript">
$(window).scroll(function() {

    if($(this).scrollTop() > 400) {
        $('.scroll-top').fadeIn(200);
    } else {
        $('.scroll-top').fadeOut(200);
    }
});

$('.scroll-top').bind('click', function(e) {
    e.preventDefault();
    $('body,html').animate({scrollTop:0},200);
});
</script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
